"""
API Endpoints
=============
Routes for the objective answer sheet evaluation pipeline.
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks, UploadFile, File
from models import (
    Student, Submission, EvaluationResult,
    AnswerKey, StudentResult, PipelineSummary, SheetUpdateSummary,
    ProcessFolderRequest, ExportToSheetsRequest, FullPipelineRequest,
)
from services.drive_service import DriveService
from services.ocr_service import OCRService
from services.evaluation_service import EvaluationService
from services.answer_key_service import AnswerKeyService
from services.sheets_service import SheetsService
from database import Database
import uuid
import os
import json
import tempfile
import shutil
from typing import Optional, List, Dict

router = APIRouter()
db = Database()
drive_service = DriveService()
ocr_service = OCRService()
eval_service = EvaluationService()
answer_key_service = AnswerKeyService()
sheets_service = SheetsService()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  In-memory state for the current exam session
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# The currently loaded answer key (set via drive extraction or manual upload)
_current_answer_key: Optional[AnswerKey] = None
# Scored results from the latest pipeline run
_current_results: List[Dict] = []
RESULTS_FILE = "current_results.json"

def save_results_to_disk():
    """Save current results to disk for persistence."""
    try:
        # Convert StudentResult objects to dictionaries for JSON serialization
        serializable_results = [r.model_dump() if isinstance(r, StudentResult) else r for r in _current_results]
        with open(RESULTS_FILE, 'w') as f:
            json.dump(serializable_results, f)
        print(f"ğŸ’¾ Saved {len(_current_results)} results to {RESULTS_FILE}")
    except Exception as e:
        print(f"âš ï¸ Failed to save results: {e}")

def load_results_from_disk():
    """Load results from disk if they exist."""
    global _current_results
    if os.path.exists(RESULTS_FILE):
        try:
            with open(RESULTS_FILE, 'r') as f:
                loaded_data = json.load(f)
                # Convert dictionaries back to StudentResult objects if necessary
                _current_results = [StudentResult(**item) if isinstance(item, dict) else item for item in loaded_data]
            print(f"âœ… Loaded {len(_current_results)} results from {RESULTS_FILE}")
        except Exception as e:
            print(f"âš ï¸ Failed to load results: {e}")


# Try to load answer key from disk on startup
if _current_answer_key is None:
    _current_answer_key = answer_key_service.load_from_disk()
    if _current_answer_key:
        print(f"âœ… Loaded persisted answer key with {_current_answer_key.total_questions} questions.")

# Try to load results from disk on startup
if not _current_results:
    load_results_from_disk()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  HEALTH & STATUS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@router.get("/status")
def get_status():
    global _current_answer_key, _current_results
    # Try reloading if missing (e.g. if server restarted)
    if _current_answer_key is None:
        _current_answer_key = answer_key_service.load_from_disk()
    
    # Reload results if empty
    if not _current_results:
        load_results_from_disk()

    return {
        "status": "Service operational",
        "answer_key_loaded": _current_answer_key is not None,
        "answer_key_questions": _current_answer_key.total_questions if _current_answer_key else 0,
        "results_count": len(_current_results),
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PHASE 1 â€” DRIVE FOLDER INGESTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@router.post("/sync-drive")
def sync_drive(folder_id: str):
    """
    Lists all files in a Google Drive folder (legacy endpoint â€” images only).
    """
    args = {"folder_id": folder_id} # generic kwargs not supported by extract_folder_id directly if detected differently
    folder_id = DriveService.extract_folder_id(folder_id)
    files = drive_service.list_all_files_in_folder(folder_id)
    
    # Auto-detect and load answer key
    global _current_answer_key
    answer_key_files, student_sheets = drive_service.separate_files(files)
    
    if answer_key_files:
        print(f"ğŸ”„ Auto-loading answer key from: {answer_key_files[0]['name']}")
        try:
            temp_dir = tempfile.mkdtemp(prefix="ak_sync_")
            local_path = drive_service.download_answer_key(answer_key_files[0], temp_dir)
            mime_type = answer_key_files[0].get("mimeType", "")
            _current_answer_key = answer_key_service.extract_answer_key(local_path, mime_type)
            print(f"âœ… Answer Key Loaded! Support for {_current_answer_key.total_questions} questions.")
            shutil.rmtree(temp_dir, ignore_errors=True)
        except Exception as e:
            print(f"âš ï¸ Failed to auto-load answer key: {e}")

    return {"files_found": len(student_sheets), "files": student_sheets}


@router.post("/scan-drive-folder")
def scan_drive_folder(request: ProcessFolderRequest):
    """
    Scans a Drive folder and separates answer key from student sheets.
    Does NOT process anything â€” just shows what's in the folder.
    """
    folder_id = DriveService.extract_folder_id(request.folder_url)
    all_files = drive_service.list_all_files_in_folder(folder_id)

    if not all_files:
        raise HTTPException(status_code=404, detail="No files found in the Drive folder.")

    answer_key_files, student_sheets = drive_service.separate_files(all_files)

    return {
        "total_files": len(all_files),
        "answer_key_files": answer_key_files,
        "student_sheets": student_sheets,
        "answer_key_count": len(answer_key_files),
        "student_sheet_count": len(student_sheets),
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PHASE 2A â€” ANSWER KEY MANAGEMENT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@router.get("/answer-key")
def get_current_answer_key():
    """Returns the currently loaded answer key."""
    global _current_answer_key
    if _current_answer_key is None:
        _current_answer_key = answer_key_service.load_from_disk()
        
    if _current_answer_key is None:
        raise HTTPException(status_code=404, detail="No answer key is currently loaded.")
    return _current_answer_key.model_dump()


@router.post("/answer-key/extract-from-drive")
def extract_answer_key_from_drive(request: ProcessFolderRequest):
    """
    Extracts the answer key from a Drive folder.
    Looks for a file named 'answer_key' (case-insensitive).
    """
    global _current_answer_key

    folder_id = DriveService.extract_folder_id(request.folder_url)
    all_files = drive_service.list_all_files_in_folder(folder_id)

    if not all_files:
        raise HTTPException(status_code=404, detail="No files found in the Drive folder.")

    answer_key_files, _ = drive_service.separate_files(all_files)

    if not answer_key_files:
        raise HTTPException(
            status_code=404,
            detail=(
                "No answer key file found. "
                "Please name your answer key file with 'answer_key' in the name "
                "(e.g., 'answer_key.csv', 'Answer_Key.xlsx', 'answer_key.pdf')."
            )
        )

    # Use the first answer key file found
    ak_file = answer_key_files[0]
    if len(answer_key_files) > 1:
        print(f"âš ï¸  Multiple answer key files found, using: {ak_file['name']}")

    # Download and parse
    temp_dir = tempfile.mkdtemp(prefix="ak_")
    try:
        local_path = drive_service.download_answer_key(ak_file, temp_dir)
        mime_type = ak_file.get("mimeType", "")
        _current_answer_key = answer_key_service.extract_answer_key(local_path, mime_type)

        return {
            "message": "Answer key extracted successfully",
            "source_file": ak_file["name"],
            "total_questions": _current_answer_key.total_questions,
            "answer_key": _current_answer_key.model_dump(),
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to extract answer key: {str(e)}")
    finally:
        shutil.rmtree(temp_dir, ignore_errors=True)


@router.post("/answer-key/upload")
async def upload_answer_key(file: UploadFile = File(...)):
    """
    Manual upload of an answer key file.
    Supports: CSV, XLSX, PDF, PNG, JPG, TXT
    """
    global _current_answer_key

    temp_dir = tempfile.mkdtemp(prefix="ak_upload_")
    try:
        # Save uploaded file
        local_path = os.path.join(temp_dir, file.filename)
        with open(local_path, "wb") as f:
            content = await file.read()
            f.write(content)

        _current_answer_key = answer_key_service.extract_answer_key(
            local_path, file.content_type
        )

        return {
            "message": "Answer key uploaded and parsed",
            "filename": file.filename,
            "total_questions": _current_answer_key.total_questions,
            "answer_key": _current_answer_key.model_dump(),
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to parse answer key: {str(e)}")
    finally:
        shutil.rmtree(temp_dir, ignore_errors=True)


@router.post("/answer-key/set-manual")
def set_answer_key_manual(answers: dict):
    """
    Manually set the answer key via JSON body.
    
    Expected format:
    {
        "answers": {"1": "A", "2": "C", "3": "B", ...},
        "marks_per_question": 1,
        "negative_marking": 0
    }
    """
    global _current_answer_key
    from models import AnswerKeyEntry

    raw_answers = answers.get("answers", {})
    marks = float(answers.get("marks_per_question", 1.0))
    negative = float(answers.get("negative_marking", 0.0))

    parsed = {}
    for k, v in raw_answers.items():
        q_num = int(k)
        parsed[q_num] = AnswerKeyEntry(correct_option=str(v).strip().upper(), marks=marks)

    _current_answer_key = AnswerKey(
        total_questions=len(parsed),
        answers=parsed,
        negative_marking=negative,
        metadata={"source": "manual_input"}
    )
    
    # Save manually set key too
    answer_key_service.save_to_disk(_current_answer_key)

    return {
        "message": "Answer key set manually",
        "total_questions": _current_answer_key.total_questions,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PHASE 2B â€” PROCESS STUDENT SHEETS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@router.post("/process-drive-folder")
def process_drive_folder(request: ProcessFolderRequest):
    """
    Main endpoint: Process all student answer sheets in a Drive folder.
    
    1. Lists all files in the folder
    2. Separates answer key from student sheets
    3. Extracts answer key (if not already loaded)
    4. OCR processes each student sheet
    5. Scores each against the answer key
    6. Returns all results
    """
    global _current_answer_key, _current_results

    folder_id = DriveService.extract_folder_id(request.folder_url)
    all_files = drive_service.list_all_files_in_folder(folder_id)

    if not all_files:
        raise HTTPException(status_code=404, detail="No files found in the Drive folder.")

    answer_key_files, student_sheets = drive_service.separate_files(all_files)

    # Step 1: Extract answer key if not already loaded - Try loading from disk first
    if _current_answer_key is None:
        _current_answer_key = answer_key_service.load_from_disk()
    
    if _current_answer_key is None:
        if not answer_key_files:
            raise HTTPException(
                status_code=400,
                detail=(
                    "No answer key loaded and no answer_key file found in the folder. "
                    "Please upload an answer key first via /api/answer-key/upload or "
                    "include a file named 'answer_key' in your Drive folder."
                )
            )

        temp_dir = tempfile.mkdtemp(prefix="ak_")
        try:
            local_path = drive_service.download_answer_key(answer_key_files[0], temp_dir)
            mime_type = answer_key_files[0].get("mimeType", "")
            _current_answer_key = answer_key_service.extract_answer_key(local_path, mime_type)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to extract answer key: {str(e)}")
        finally:
            shutil.rmtree(temp_dir, ignore_errors=True)

    if not student_sheets:
        raise HTTPException(
            status_code=404,
            detail="No student answer sheets found in the folder (only the answer key was found)."
        )

    # Step 2: Process each student sheet
    results = [] # Use a local list to accumulate results
    errors = []
    temp_dir = tempfile.mkdtemp(prefix="sheets_")

    try:
        for idx, sheet_file in enumerate(student_sheets):
            file_name = sheet_file["name"]
            file_id = sheet_file["id"]
            print(f"\n{'='*50}")
            print(f"Processing [{idx+1}/{len(student_sheets)}]: {file_name}")

            try:
                # Download
                local_path = os.path.join(temp_dir, file_name)
                download_ok = drive_service.download_file(file_id, local_path)
                if not download_ok:
                    errors.append({"file": file_name, "error": "Download failed"})
                    continue

                # OCR Extract
                extracted = ocr_service.extract_objective_sheet(local_path)
                if "error" in extracted:
                    errors.append({"file": file_name, "error": extracted["error"]})
                    continue

                # Score
                student_result = EvaluationService.match_and_score(
                    _current_answer_key, extracted
                )
                results.append(student_result) # Add to local list

                print(f"  âœ… Evaluated {student_result.entry_number} â€” Score: {student_result.total_score}/{student_result.max_score}")
                
                # SAVE DEBUG JSON per student
                debug_dir = "debug_evals"
                os.makedirs(debug_dir, exist_ok=True)
                debug_file = os.path.join(debug_dir, f"{student_result.entry_number}.json")
                with open(debug_file, "w") as f:
                    json.dump(student_result.model_dump(), f, indent=2)
                print(f"     saved debug JSON to {debug_file}")

                # Also save to DB
                # 1. Student
                db.add_student(Student(
                    id=student_result.entry_number,
                    name=student_result.name,
                    roll_number=student_result.entry_number
                ))
                
                # 2. Submission
                submission_id = file_id  # simple mapping
                db.add_submission(
                    Submission(
                        id=submission_id,
                        student_id=student_result.entry_number,
                        exam_id="default_exam",
                        file_id=file_id,
                        status="processed"
                    ),
                    extracted_data=extracted
                )
                
                # 3. Result (Score + Details + Comments)
                details_list = [d.model_dump() for d in student_result.details]
                db.add_result(
                    EvaluationResult(
                        submission_id=submission_id,
                        score=student_result.total_score,
                        feedback=student_result.comments
                    ),
                    details=details_list
                )

            except Exception as e:
                errors.append({"file": file_name, "error": str(e)})
                print(f"  âŒ Error: {e}")

    finally:
        shutil.rmtree(temp_dir, ignore_errors=True)
    
    # Update global state and persist
    _current_results = results
    save_results_to_disk()

    return PipelineSummary(
        total_students_processed=len(_current_results),
        answer_key_source=_current_answer_key.metadata.get("source_file", "loaded"),
        results=_current_results,
        errors=errors,
    ).model_dump()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PHASE 3 â€” GOOGLE SHEETS EXPORT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@router.post("/export-to-sheets")
def export_to_sheets(request: ExportToSheetsRequest):
    """
    Matches evaluated results with a Google Sheet student list and writes marks.
    
    The Google Sheet should have columns for: Entry Number, Name, Marks
    Column headers are auto-detected.
    
    Entry numbers are matched using the format yyyybbbnnnn (e.g. 2023CSB1122).
    Names are cross-verified â€” mismatches are flagged but marks are still written.
    """
    # Try in-memory results first, fall back to database
    global _current_results
    results_dicts = []
    
    if _current_results:
        # Robustly handle both StudentResult objects and dictionaries
        results_dicts = []
        for r in _current_results:
            if hasattr(r, "model_dump"):
                results_dicts.append(r.model_dump())
            elif isinstance(r, dict):
                results_dicts.append(r)
            else:
                # Fallback for unexpected types?
                continue
    else:
        # Pull from database
        try:
            db_results = db.get_all_results()
            if db_results:
                for row in db_results:
                    entry = row.get("student_id", "")
                    score = row.get("score", 0)
                    
                    # Parse details
                    details_raw = row.get("details")
                    details = []
                    if details_raw:
                        try:
                            details = json.loads(details_raw) if isinstance(details_raw, str) else details_raw
                        except (json.JSONDecodeError, AttributeError):
                            details = []
                    
                    if entry and entry != "temp_unknown":
                        results_dicts.append({
                            "entry_number": entry,
                            "name": "", # Name usually not in results table join, simplifying
                            "total_score": score,
                            "comments": row.get("feedback", ""),
                            "details": details,
                        })
        except Exception as e:
            print(f"âš ï¸ DB Fallback failed: {e}")

    if not results_dicts:
        raise HTTPException(
            status_code=400,
            detail="No results to export. Process some answer sheets first."
        )

    try:
        summary = sheets_service.update_marks(request.sheet_url, results_dicts)
        
        # Clear detailed results from memory as requested
        if _current_results:
            print("ğŸ—‘ï¸  Cleared in-memory results after successful export.")
            _current_results = []
            
        return summary

    except RuntimeError as e:
        raise HTTPException(status_code=503, detail=str(e))
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Sheet export failed: {str(e)}")


@router.get("/sheets/preview")
def preview_sheet(sheet_url: str):
    """
    Preview a Google Sheet â€” shows detected columns and student list.
    Does NOT write anything.
    """
    try:
        data = sheets_service.read_student_list(sheet_url)
        return {
            "sheet_name": data["sheet_name"],
            "columns_detected": data["columns"],
            "student_count": len(data["students"]),
            "students": data["students"][:20],  # preview first 20
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  FULL PIPELINE (ONE-CLICK)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@router.post("/full-pipeline")
def run_full_pipeline(request: FullPipelineRequest):
    """
    One-click pipeline:
    Drive folder â†’ Extract answer key â†’ OCR all sheets â†’ Score â†’ Write to Google Sheets
    """
    # Step 1 & 2: Process drive folder (extracts key + scores students)
    folder_result = process_drive_folder(
        ProcessFolderRequest(folder_url=request.drive_folder_url)
    )

    # Step 3: Export to sheets
    try:
        sheet_result = export_to_sheets(
            ExportToSheetsRequest(sheet_url=request.sheets_url)
        )
    except HTTPException as e:
        sheet_result = {"error": e.detail}

    return {
        "pipeline": folder_result,
        "sheet_export": sheet_result,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  RESULTS & DATA ACCESS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@router.get("/results")
def get_results():
    """Get all results from the current session."""
    if _current_results:
        return {
            "source": "current_session",
            "count": len(_current_results),
            "results": [r.model_dump() for r in _current_results],
        }
    # Fallback to DB
    return {
        "source": "database",
        "results": db.get_all_results(),
    }


@router.delete("/results/clear")
def clear_results():
    """Clear current session results and answer key."""
    global _current_answer_key, _current_results
    _current_answer_key = None
    _current_results = []
    
    # Also remove persisted files
    if os.path.exists("current_answer_key.json"):
        os.remove("current_answer_key.json")
    if os.path.exists(RESULTS_FILE):
        os.remove(RESULTS_FILE)
        
    return {"message": "Session cleared"}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  LEGACY ENDPOINTS (kept for compat)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@router.post("/process-file")
def process_file(file_id: str, file_name: str, background_tasks: BackgroundTasks):
    """
    Legacy: Downloads and processes a single file (OCR + Evaluation).
    """
    submission_id = str(uuid.uuid4())
    submission = Submission(
        id=submission_id,
        student_id="temp_unknown",
        exam_id="exam_001",
        file_id=file_id,
        status="processing"
    )
    db.add_submission(submission)
    background_tasks.add_task(_legacy_process_task, submission_id, file_id, file_name)
    return {"message": "Processing started", "submission_id": submission_id}


def _legacy_process_task(submission_id: str, file_id: str, file_name: str):
    """Legacy background task for processing a single file."""
    try:
        temp_path = f"temp_{file_name}"
        download_success = drive_service.download_file(file_id, temp_path)

        if not download_success and not os.path.exists(temp_path):
            print(f"Failed to download {file_id}")
            return

        # Use specialized objective sheet extraction (robust prompt)
        extracted_data = ocr_service.extract_objective_sheet(temp_path)
        if "error" in extracted_data:
            print(f"OCR Error: {extracted_data['error']}")
            return

        roll_no = extracted_data.get("roll_number") or "unknown"
        student_name = extracted_data.get("student_name") or "unknown"
        db.add_student(Student(id=str(roll_no), name=str(student_name), roll_number=str(roll_no)))

        updated_sub = Submission(
            id=submission_id,
            student_id=roll_no,
            exam_id="exam_001",
            file_id=file_id,
            status="evaluated"
        )
        db.add_submission(updated_sub, extracted_data)

        # If we have an answer key, use the new scoring
        if _current_answer_key:
            # Note: extract_objective_sheet already returns normalized structure (answers dict)
            # Use it directly. ocr_service._normalize_objective_output handles dict input too.
            normalized = ocr_service._normalize_objective_output(extracted_data)
            
            student_result = EvaluationService.match_and_score(
                _current_answer_key, normalized
            )
            
            # CRITICAL: Append to current session results for Export feature!
            _current_results.append(student_result.model_dump())
            save_results_to_disk()

            result_record = EvaluationResult(
                submission_id=submission_id,
                score=student_result.total_score,
                feedback=f"{student_result.correct_count} correct, "
                         f"{student_result.incorrect_count} incorrect, "
                         f"{student_result.unattempted_count} unattempted"
            )
            db.add_result(result_record, student_result.model_dump())
        else:
            print(f"âš ï¸  No answer key loaded for file {file_name}. Skipping scoring.")
            result_record = EvaluationResult(
                submission_id=submission_id,
                score=0,
                feedback="No answer key loaded. Score: 0/0",
                details=json.dumps({"error": "No answer key loaded"})
            )
            db.add_result(result_record, {"error": "No answer key loaded"})

        if os.path.exists(temp_path):
            os.remove(temp_path)

    except Exception as e:
        print(f"Error processing submission {submission_id}: {e}")
